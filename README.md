# cuda_learning 入门
记录学习Cuda编程的项目

## One Day:

任务清单：配置GPU环境以及创建GPU版的Hello world!

### 一、配置GPU编程环境

主要参考[这篇文章](https://blog.csdn.net/chen565884393/article/details/127905428)

首先检查N卡的驱动，并安装对应的CUDA，然后安装CUDNN

因为我的是GTX1060，所以只能支持CUDA11.4，注意这里我才过坑，安装了11.5,11.6版本在后面对cu程序进行编译的时候会报错，因此要注意显卡能支持的最高CUDA版本，然后根据CUDA版本确定 Visual Studio的版本，

因为官网默认下载最新版本，实际上是不支持古早的CUDA版本的，因此可以根据上面文章提到连接查询官方CUDA支持的VS范围，**注意此处的版本十分重要，如果版本错误会导致后面编译失败**

我最开始下载的是VS 2022 community 的版本，即使在里面选择安装2019,2017的编译器仍然会报错误，提示头文件找不到，或者版本的问题，改回2019版本的就没问题了。

### 二、Hello world

对于GPU编程主要分为两部分，一部分是主机(host)，另一部分则是设备(Device)

主机代码主要在CPU上运行，设备代码则是在GPU上运行完成

我们可以在CPU上运行逻辑分支，然后调用GPU进行计算，但这里GPU的计算与主机是异步的。

调用GPU的函数之后，CPU的控制权依然会往下运行，如果CPU后面的代码可以主动调用同步操作`cudaDeviceSynchronize`函数来同步，这里CPU会阻塞等待GPU完成计算。

同时也可以调用`cudaMemcpy`在主机和设备间拷贝数据时，这里CPU也会阻塞来隐式同步数据。

## Two Day:

GPU编程主要语法与C语言类似，主要是多了一些特殊标识符

$$
\begin{array}{c|c|c|c}
\hline
限定符 & 执行 & 调用 & 备注\\
\_\_{global}\_\_ & 设备端执行 & 主机调用，也可以从算力3以上的设备中调用 & 返回类型必须是void \\
\_\_{device}\_\_ & 设备端执行 & 仅能从设备中调用  \\
\_\_{host}\_\_ & 主机端执行 & 仅能从主机调用 & 可省略 \\ 
\hline
\end{array}
$$

### 代码要点

对于GPU调用时，其线程创建模型为两个部分，第一个部分叫网格(grid),第二个叫线程块(block),线程块包含了N个线程，结构图如下：

```
                                            每一个Grid
----------------------------      -------------------------------
|        |        |        |      |         |         |         |
| grid1  | grid2  | grid3  |      | block1  | block2  | block3  |
|--------------------------|      |-----------------------------|
|        |        |        |      |         |         |         |
|        |        |        | -->  |         |         |         |
|--------------------------|      |-----------------------------|
|        |        |        |      |         |         |         |
|        |        |        |      |         |         |         |
----------------------------      -------------------------------
```
Grid和Block都由一个三维坐标标识(dim3)，每个`(x,y,z)` 唯一确定一个网格或者一个块。

因此我们可以创建一个维度`(3,3,3)`的网格，每个网格又是一个维度为`(4,4,4)`的线程块。因此总线程数为 $3^3(网格数) * 4^3(线程数)$

1. 网格的维度由线程块的数量来表示
2. 线程块的维度由线程数来表示   

在代码`MatrixSum`中：
```
dim3 block = (16);
dim3 grid = ((matrix_len + block.x - 1) / block.x);
```
这里计算所需要网格数的代码`((matrix_len + block.x - 1) / block.x)` 是为了让所有的数据能被网格所包括，因此加上`block.x-1`,这也是为了规避除法的向下取整，导致无法囊括到所有数据。

```
------------------------------------------------------------
| data1 | data2 |       |       |       |       |       |  |
|       |       |       |       |       |       |       |  |
------------------------------------------------------------
                                                            ^
                                                            |
-----------------------------------------------------------------
| block1| block2|       |       |       |       |       |       |
|       |       |       |       |       |       |       |       |
-----------------------------------------------------------------
    1       2       3       4       5       6       7       8
```

# GPU优化

## 3.1GPU架构

GPU有多个流式多处理器(SM)，执行时每一个SM分配多个线程块，SM分配的线程块数由其中的资源来决定

CUDA中采用单指令多线程架构(`SIMT`)，每32个线程为一组称为线程束(`warp`)
线程束中的所有线程同时执行相同的指令

每一个SM都将分配给他的所有线程块的线程按照线程束进行划分，其中一个线程块被分配到一个SM后会一直存在其中，直到完成线程任务

```     
               -------    -------------
           |——>|warp | -->| 32 thread |  --|     ---------
           |   -------    -------------    |---> | block |
-------    |   -------    -------------    |     ---------
| SM  |  --|——>|warp | -->| 32 thread |  --|
-------    |   -------    -------------
           |   -------    -------------
           |——>|warp | -->| 32 thread |
               -------    -------------
```

**warp才是SM上的执行单位**，因此不同线程束之间的进度可能会不一致，也就会导致线程块之间不同的线程以不同的速度前进。

编译指令：
```
nvcc -O3 按照级别3来优化主机代码
nvcc -G -g 生成Debug信息
```

## 3.2线程束

在硬件上线程都是一维排开，尽管线程块可能是1,2,3维。同时根据`threadIdx.x`的连续值来划分线程束，对于多维的线程块，同样可以根据下列公式转换成一维排列。

一维线程 `(x)`：
$$
thread_{i} = threadIdx.x
$$

二维线程 `(x,y)`：
$$
thread_{i} = threadIdx.x + threadIdx.y \times blockDim.x
$$

三维线程块 `(x,y,z)`
$$
thread_{i} = threadIdx.z \times  (blockDim.x \times blockDim.y) + threadIdx.y \times blockDim.x + threadIdx.x
$$

可以看到线程坐标按照 x为内部维度，y作为外部维度，z作为最外面的维度进行递增。

注意到：$一个线程块中的线程束的数量 = 向上取整({一个线程块中的线程数量 \over 线程束大小})$

因此**线程束不会跨线程块进行分配**，如果当一个线程块的线程数不是线程束大小的偶数倍会造成资源浪费。并会影响线程束的调用，从而影响效率

### 3.2.1线程束分化

GPU对于带有逻辑判断的分支语句，例如`if...else...`并没有复杂的分支预测的能力。*但对于同一个线程束中所有线程必须在同一周期中执行相同的指令*，因此分支语句会导致一个线程束中不同的线程走到不同的分支语句，这就是线程束的分化。

**线程束的分化会明显导致性能下降**

对于代码：`chapter3/branch.cu`，关闭编译器的分支预测优化功能进行运行测试。
编译：`nvcc -g -G branch.cu -o branch`

运行：`sudo ./ncu --metrics smsp__sass_average_branch_targets_threads_uniform.pct branch`

可以看到在RTX3050上分支效率结果为：
```
mathKernal2:80%
mathKernal3:100%
mathKernal4:71.43%
```
线程束级别的分支效率几乎没有分化的分支

每个线程束的资源都保存在SM单元中，进行线程束切换的时候不会产生性能损失

同时每个SM内核只有固定数量的共享内存和寄存器组，
因此每个线程消耗的寄存器越少，SM能同时处理的线程束越多
每个线程块消耗的共享内存越多，SM能同时处理的线程块就越少

### 3.2.2 延迟隐藏
指令发出和完成这段时钟周期被称为指令延迟

在指令延迟这段时间中，每个调度器都有一个符合条件的线程束可用于执行，那么就能隐藏这段延迟（流水线）

延迟分为：算术指令延迟/内存指令延迟

隐藏延迟所需要的活跃线程束数量可以由如下公式进行计算：$线程束数量 = 线程束的延迟\times吞吐量$

吞吐量由SM中每个周期的操作数确定，SM中以线程束为执行单位，因此一个周期会同时有线程束大小的操作执行(warpSize)，上述式子换算成操作单位可以表示成：$操作数 = 指令延迟\times吞吐量$

吞吐量的单位为：操作/周期
指令延迟单位为：周期

根据操作数的需要同时很容易反推需要多少个线程，线程束以及每个SM至少多少个线程束。

同时不要忽略SM的线程束数量同时也受限于硬件资源

### 3.2.3 占用率

占用率的计算公式如下：
$$
占用率 = {活跃线程束 \over 最大线程束}
$$

最大线程束数量可以在设备信息中查询，通过`cudaGetDeviceProperties`获取设备参数信息的结构体，`maxThreadsPerMultiProcessor/warpSize` 可以得到最大的线程束数量

例如本机为`NVIDIA GeForce RTX 3050 Laptop GPU`,其中最大的线程束数量为`maxThreadsPerMultiProcessor/warpSize = 1536/32 = 48`

可以对编译器添加参数`--ptxas-options=-v`
或在CMakeLists.txt中添加`set(CUDA_NVCC_FLAGS --ptxas-options=-v)` 来查看每个核函数使用了多少个寄存器以及共享内存的资源。

提高利用率我们需要设置合适的线程块配置。过大过小都会影响资源的利用率。

小线程块：会在所有资源被充分利用之前到达SM的线程束数量的限制

大线程块：会导致每个SM中每个线程可用的硬件资源过少

网格和线程块大小划分准则：

1. 保持每个块中的线程是**线程束大小的倍数**
2. 避免块太小，每个块至少有表128或者256个线程
3. 根据内核资源调整块的大小
4. 块的数量要远多于SM的数量，达到足够的并行

### 3.2.4 同步

线程块之间的同步: `__device void __syncthreads()`
系统级同步：`cudaDevicesSynchronize()`

线程块是以线程束为单位执行，因此同一个线程块中的不同线程束会处于不同的程序点，所以提供`__syncthreads`来同步块中的所有线程

同步之前线程块中的所有线程产生的共享内存或全局内存的修改操作等，在同步后会对线程块中的所有线程可见，并且是安全的。**因此可用于线程间的通讯**



## 3.3 性能指标

老版本cuda可以使用nvprof来查看程序运行的各种指标,现在逐渐替换成ncu工具来检查程序运行的性能指标。

更多参数对照可以参考官方提供的CUDA手册`《NSIGHT COMPUTE COMMAND LINE INTERFACE》` [Ref:](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-guide)


### 3.3.1 SM占用率

```
nvprof --metrics achieved_occupancy ./xxx
ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active ./xxx
```
该指标越高越好

核函数运行时间
```
nvprof ./xxx
nsys nvprof ./xxx
```

在矩阵求和的例子中，不同参数下SM占用率和耗时分别为：
```
(32,32):53.24%  22.10ms
(32,16):70.08%  21.60ms
(16,32):73.10%  21.04ms
(16,16):78.04%  21.19ms
```

从上面结果可见第二种比第一种有更多的块，有着更好性能，但第四中比第三种也有更多的块，但性能并没有更好的提升，虽然SM占用率较高

### 3.3.2 内存读取效率

```
./ncu --metrics l1tex__t_bytes_pipe_lsu_mem_global_op_ld.sum.per_second ./xxx
```

可以看到结果如下：
```
(32,32):78.99 Gb/s 
(32,16):89.97 Gb/s
(16,32):90.65 Gb/s
(16,16):91.21 Gb/s
```
其中第四中具有最高的内存读取效率，但运行时间并没有第三种情况快，所以高内存读取效率并不代表着有最好的性能

### 3.3.3 全局加载效率

```
./ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./xxx
```
可以看到结果如下：
```
(32,32):100%
(32,16):100%
(16,32):100%
(16,16):100%
```

我们可以结合上述三个指标来判断最佳的块组合
